{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Jupyter Notebook 1: Text Classifiction \n",
    "### Based on Tensorflow Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Data and Software\n",
    "\n",
    "* Let's start by making a very simple neural network, fit it on some data and asses its performance using what we learned. \n",
    "\n",
    "* This lecture is based on the tutorials provided on the Tensorflow website. Tensorflow provides a series of example datasets that you can download and play around with. We will play with two today, the Higgs dataset (sims) and the IMDB dataset. Both of these are large enough datasets (50,000 movie reviews and 500,000 events). I recommend playing with both.\n",
    "\n",
    "* We'll skip over regression, as text classification will only require one more conceptual jump. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The goal\n",
    "* We are going to try and predict whether a movie review is positive or negative just by reading it. This is a binary classification problem (although it could be easily adapted to a regression problem). We have a prelabelled dataset which we will download. How would you do this if you didn't know machine learning existed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Packages\n",
    "* We will import the basic os integration, and numpy. \n",
    "* We also need tensorflow (the central package) and keras which is neural network utilities. Keras can be used on its own but it is most common to use it in the tensorflow framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:21:29.115537Z",
     "iopub.status.busy": "2020-10-14T01:21:29.114880Z",
     "iopub.status.idle": "2020-10-14T01:21:36.078505Z",
     "shell.execute_reply": "2020-10-14T01:21:36.078948Z"
    },
    "id": "8RZOuS9LWQvv",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 15:51:14.383037: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAsKG535pHep",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Download the IMDB dataset and Organize\n",
    "\n",
    "* This next block downloads all the data. Normally you will be using your own data but tensorflow is particular on the organization of data. Watch out for database objects in the beginner tutorials. Getting data out of these objects is unintuitive. \n",
    "\n",
    "* The data will already be organized into a training set and a test set so let’s look at this along with the structure of the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:21:36.093390Z",
     "iopub.status.busy": "2020-10-14T01:21:36.092701Z",
     "iopub.status.idle": "2020-10-14T01:21:50.250856Z",
     "shell.execute_reply": "2020-10-14T01:21:50.250216Z"
    },
    "id": "k7ZYnuajVlFN",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "                                    untar=True, cache_dir='.',\n",
    "                                    cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "train_dir = os.path.join(dataset_dir, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysMNMI1CWDFD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Structure\n",
    "\n",
    "* This data is already sorted into a logical hierarchy split into an equal sized training and testing data set. This is a process you would need to do yourself. You can see that the labels come from the folder. They are sorted into positive and negative reviews.\n",
    "* Each of the actual data files is a test file with a positive or negative review. We can import these simply as strings. The data itself contains punctuation and html keys so how do we handle this?\n",
    "* The tensor flow hierarchy of files is:\n",
    " ```\n",
    "main_directory/\n",
    "...class_a/\n",
    "......a_text_1.txt\n",
    "......a_text_2.txt\n",
    "...class_b/\n",
    "......b_text_1.txt\n",
    "......b_text_2.txt\n",
    "```\n",
    "\n",
    "* The data downloaded also has an 'unsup' folder which we will remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:21:50.273556Z",
     "iopub.status.busy": "2020-10-14T01:21:50.272892Z",
     "iopub.status.idle": "2020-10-14T01:21:50.275748Z",
     "shell.execute_reply": "2020-10-14T01:21:50.275147Z"
    },
    "id": "R7g8hFvzWLIZ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directed by Brian De Palma and written by Oliver Stone, \"Scarface\" is a movie that will not be forgotten. A Cuban refugee named Tony Montana (Pacino) comes to America for the American Dream. Montana then becomes the \"king\" in the drug world as he ruthlessly runs his empire of crime in Miami, Florida. This gangster movie is very violent, and some scenes are unpleasant to watch. This movie has around 180+ F-words and is almost three hours long. This movie is entertaining and you will never get bored. You cheer for the Drug-lord, and in some scenes you find out that Montana isn't as evil as some other Crime Lords. This is a masterpiece and i recommend that you see this. You will not be disappointed. 9/10\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Dick Tracy is easily the best comic book based movie made to date. The movie has the same feel as the comic book, staying true to the color scheme. The Batman series has climbed, fallen, climbed and fallen again. Dick Tracy has true staying power as something that both adults and children can enjoy. The good guys triumph over evil, without blood and gore to get the point across. Al Pacino does a wonderful job of his own adaptation of Big Boy Caprice and Madonna is memorable as Breathless. But the best job by far is Warren Beatty who just epitomizes Dick Tracy just as he did with Clyde Barrows. I can't wait until it comes out on DVD on April 2, 2002, my tape is wearing thin.\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Having seen the hot Eliza Dushku in the pretty good Wrong Turn, I decided to pick this one up instead of Return of the Living Dead, of all movies. Haven't seen that one yet, but, considering it is one of the most highly acclaimed horror movies ever, safe to say I made the wrong choice. There is simply nothing to recommend this movie, and I am talking about the supposedly superior killer cut. It didn't even have the youthful sex appeal of mediocre to poor movies like I Know What You Did Last Summer or Valentine or Urban Legend. It simply made no sense, held no excitement, had very little interesting acting or compelling writing. The release date was apparently put off numerous times for about a year running, and the reason is obvious. The whole movie comes off as a bunch of meaningless scenes thrown together haphazardly, to meaningless effect. Get Wrong Turn instead, if you want to see Dushku. I would like to see a movie with her and the super-hot Elisabeth Harnois--but I don't think even that would have made this movie watchable. Casey Affleck, so promising in Good Will Hunting, is awful here--he seems to lack both intelligence and guts. That's enough on this one.\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Simply awful slasher, molded from the I KNOW WHAT YOU DID LAST SUMMER type of fodder, has beautiful wealthy college students spending spring break in a Florida condo being murdered one by one. A misfire in every category imaginable from properly built suspense to the executed death sequences..nothing is handled properly and the characters leave little more than caricatures you root to see decimated as quick as possible. Del Tenney(The Curse of the Living Corpse;I Eat Your Skin), of all people, executive produced, co-wrote, and stars as a priest in a pivotal role whose relationship to the killer I guess means something to why he's psychotic. The revelation of the killer is awkwardly handled and ineffective, probably not surprising a soul who watches it. There are a lot of attempted jump scares, with one character popping out to frighten their friend, which couldn't even manufacture a few cheap thrills, because they are so calculated in such a tepid way. Most of the attacks occur off-screen with bleeding throat cuts(..or pools)representing the only real display of violence. The protracted finale, where the killer goes on and on with the actor desperately trying to make his villain as diabolical and demented as possible, is embarrassing and tense-less. There's not one single positive to derive from this clichéd and dull exercise with the pretty cast making little effort to rise past their one-dimensional roles. And, for pity sake, they could've at least allowed us to see Joey Lawrence get decapitated or something for withstanding the misery of sitting through this junk heap for 90 agonizing minutes.<br /><br />Dorie Barton, as the heroine final girl, Beth Morgan, who the killer seems to be obsessed with, couldn't be more vacuous and uninteresting(..oh, she was in rehab, and takes pills for her nightmares;such intriguing exposition). Chad Allen, cast against type as a very intimidating \"friend\" of the group(..who happens to disappear from the film first, setting up the idea that he's the first victim), has a tough hill to climb with his role, so steep he eventually stumbles, rolling uncontrollably with no end in sight. Jeff Conaway, needing some cash I guess, has the beleaguered FBI agent role, whose daughter's murder motivates him to seek out the killer, leading him to Florida. Jack McGee has his usual a$$hole role as a smart-mouth Florida Police Chief who is often nose to nose with Conaway's agent.<br /><br />Oh, the answer to the title's question..not really. Because once you get the answer, you will wonder why you bothered with this anyway.\n",
      "\n",
      "------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)\n",
    "\n",
    "samps= ['pos/11877_10.txt', 'pos/11824_10.txt', 'neg/8058_1.txt', 'neg/8047_2.txt']\n",
    "for fil in samps:\n",
    "    sample_file = os.path.join(train_dir,fil)\n",
    "    with open(sample_file) as f:\n",
    "        print(f.read())\n",
    "        print('\\n------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95kkUdRoaeMw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset Object\n",
    "* We want to create a training dataset object and to do this we will use the Kera preprocessing utility. If you can ever get your data into a format where these utilities work, it will make your life a lot easier. \n",
    "* In this step we will also split our training data into a training and validation data set (80-20 split).\n",
    "* You can call keras once or twice, but if you call it twice then you need the seed to be defined or you will shuffle twice. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:21:51.126362Z",
     "iopub.status.busy": "2020-10-14T01:21:51.125704Z",
     "iopub.status.idle": "2020-10-14T01:22:00.774888Z",
     "shell.execute_reply": "2020-10-14T01:22:00.774314Z"
    },
    "id": "nOrK-MTYaw3C",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 15:52:07.156089: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32 #Size of the batches\n",
    "seed = 42 # To make sure we get the same answer because we shuffle the input\n",
    "\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=seed)\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='validation', \n",
    "    seed=seed)\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/test', \n",
    "    batch_size=batch_size)\n",
    "raw_train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWq1SUIrp1a-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Labels\n",
    "* The Keras tool will automatically get the name of the classes from the folder names. To check which is which we can just select the appropriate \"class_names\" from the dataset object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:22:00.810496Z",
     "iopub.status.busy": "2020-10-14T01:22:00.809814Z",
     "iopub.status.idle": "2020-10-14T01:22:00.812622Z",
     "shell.execute_reply": "2020-10-14T01:22:00.812126Z"
    },
    "id": "MlICTG8spyO2",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_ds.class_names[0])\n",
    "print(raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJmTiO0IYAjm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standardize Tokenize and Vectorize\n",
    "\n",
    "* This is the stage where we clean the data and convert non-numerical data into numerical data\n",
    "\n",
    "* Standardization is a process by which we simplify the data by removing anything that may cause us problems. This is things like html codes. \n",
    "\n",
    "* Tokenization is splitting large data into tokens. In this case splitting strings into words. This is normally done with whitespace. \n",
    "\n",
    "* Vectorization converts these tokens into numerical values. We will use a `TextVectorization` layer that adapts our mapping. Vectorization and tokenizing are both handled by the same layer. You can substitute this layer with previous vectorization networks for an improved result (transfer learning). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:22:02.677457Z",
     "iopub.status.busy": "2020-10-14T01:22:02.676340Z",
     "iopub.status.idle": "2020-10-14T01:22:02.679002Z",
     "shell.execute_reply": "2020-10-14T01:22:02.678399Z"
    },
    "id": "SDRI_s_tX1Hk",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:22:06.903487Z",
     "iopub.status.busy": "2020-10-14T01:22:06.902837Z",
     "iopub.status.idle": "2020-10-14T01:22:06.930006Z",
     "shell.execute_reply": "2020-10-14T01:22:06.930422Z"
    },
    "id": "XULcm6B3xQIO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b\"This Italian film from the '70's is NOT even in the class with Dog Soldiers, The Howling, or even that awful American Werewolf in Paris, BUT...it is fun to watch. I'm talking about watching the lead actress, a stunning blonde, run amok in her birthday suit. We're talking about graphic, complete nudity...it's obvious that she is a real blonde...humma humma humma!! The story is a hoot, the SFX are childish, and the acting (for the most part) stinks. The only redeeming value of this movie is all (and there is a LOT) the nudity & sex scenes. Tame by HBO standards, but still fun to see when you find yourself without a date on Saturday night. OK...HERE'S THE SPOILER...There is NO werewolf (except in the opening scene of the heroine(??)'s ancestor. The girl just imagines that she's a werewolf...in other words, a clinical Lycanthrope.\", shape=(), dtype=string)\n",
      "Label neg\n",
      "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
      "array([[  11, 1175,   19,   35,    2,  980,    7,   21,   53,    8,    2,\n",
      "         788,   16,  890, 1245,    2, 7091,   41,   53,   12,  365,  312,\n",
      "        1823,    8, 1371,    1,    7,  244,    6,  103,  141,  657,   42,\n",
      "         146,    2,  486,  516,    4, 1298, 1941,  530, 9643,    8,   39,\n",
      "        3268, 1770,   65,  657,   42, 2086,  555,    1,  576,   12,   55,\n",
      "           7,    4,  145,    1,    1,    1,    2,   63,    7,    4, 7370,\n",
      "           2,    1,   23, 3783,    3,    2,  109,   15,    2,   88,  170,\n",
      "        4183,    2,   61, 1561, 1123,    5,   11,   17,    7,   30,    3,\n",
      "          47,    7,    4,  171,    2, 1018,  388,  136, 3959,   32, 4248,\n",
      "        1471,   18,  125,  244,    6,   67,   51,   22,  163,  612,  202,\n",
      "           4, 1290,   20, 2339,  313,    1,    2,    1,    7,   57, 1823,\n",
      "         549,    8,    2,  634,  134,    5,    2, 8660,    1,    2,  247,\n",
      "          40,    1,   12,  427,    4,    1,   78,  677,    4,    1,    1,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[1], label_batch[1]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", raw_train_ds.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XD2H6utRydGv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applying Map\n",
    "* Great now we can map all of our data to integers in a commonly shaped tensor\n",
    "* We will no apply this map\n",
    "* This isn't the last step, all we've done is an initial vectorization. None of the words carry with them connections about the other words. This step is called embedding and we will build it into our network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:22:06.985745Z",
     "iopub.status.busy": "2020-10-14T01:22:06.985011Z",
     "iopub.status.idle": "2020-10-14T01:22:07.136190Z",
     "shell.execute_reply": "2020-10-14T01:22:07.136635Z"
    },
    "id": "2zhmpeViI1iG",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsVQyPMizjuO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improving Training Speed\n",
    "\n",
    "* Neural networks are pretty slow and so anything you can do to remove bottlenecks is pretty important. \n",
    "\n",
    "* If you have a GPU, use it! \n",
    "\n",
    "* Defining a `.cache()` and a `.prefetch()` will help mitigate I/O issues. The `AUTOTUNE` option will try and calculate this for you manually but for more advanced applications you will have to tweak this yourself. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:22:07.141646Z",
     "iopub.status.busy": "2020-10-14T01:22:07.140967Z",
     "iopub.status.idle": "2020-10-14T01:22:07.145057Z",
     "shell.execute_reply": "2020-10-14T01:22:07.144472Z"
    },
    "id": "wMcs_H7izm5m",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLC02j2g-llC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Network Structure\n",
    "* We now want to define the layers of our network. The first layer will be an embeding layer. This takes up our vector and looks up the embedding which encodes the connections between words. This will train as our model trains. \n",
    "\n",
    "* The next layer is just a pooling layer. Since our input is of variable length this is necessary. This would not be required if all reviews were the same length.\n",
    "\n",
    "* Now the actual neural network (the so called `Dense` layer). This is just 16 connections linking everything from the pooling network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:22:07.166447Z",
     "iopub.status.busy": "2020-10-14T01:22:07.165656Z",
     "iopub.status.idle": "2020-10-14T01:22:07.206084Z",
     "shell.execute_reply": "2020-10-14T01:22:07.205427Z"
    },
    "id": "xpKOoWgu-llD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          160016    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features + 1, embedding_dim),\n",
    "  #layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  #layers.Dropout(0.2),\n",
    "  #layers.Dense(16,kernel_regularizer=regularizers.l2(0.001)),\n",
    "  #layers.Dropout(0.2),\n",
    "\n",
    "# layers.Dense(1,kernel_regularizer=regularizers.l2(0.001))])\n",
    "  layers.Dense(1)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the Embedding layers\n",
    "* Let's visualize what's happening with a rough PCA (principal component analysis):https://projector.tensorflow.org/\n",
    "* The next box will just save our embedding layers so we can read them into the projector. \n",
    "* The projector will select new combinations of coordinates based on the vectors we send in and should give us a rough sense of what's going on by eliminating some of the less important dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4EqVWg4-llM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss function and optimizer\n",
    "* Now we need to define our loss funciton and the optimizer. \n",
    "* We will use modified gradient descent ('adam') although other options are available ('SGD') \n",
    "* Our loss function will be the binary cross entropy defined as\n",
    "BinaryCrossEntropy= $-(y\\log(p)+(1-y)\\log(1-p))$\n",
    "* The metric is how we evaluate the accuracy of our model. In this case it will take the most likely review and check it against the real review. The metric can be different from the crossentropy and does not affect the model at all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:22:07.217829Z",
     "iopub.status.busy": "2020-10-14T01:22:07.217023Z",
     "iopub.status.idle": "2020-10-14T01:22:07.233977Z",
     "shell.execute_reply": "2020-10-14T01:22:07.234421Z"
    },
    "id": "Mr0GP-cQ-llN",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35jv_fzP-llU",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "* Now all you need to do is run through a couple gradient descents. \n",
    "* The epoch is just how many times you've run through the training set.\n",
    "* This is a fairly simple network and so each epoch ~2 s. \n",
    "* This is normally the longest step computationally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:22:07.238934Z",
     "iopub.status.busy": "2020-10-14T01:22:07.238269Z",
     "iopub.status.idle": "2020-10-14T01:22:36.278880Z",
     "shell.execute_reply": "2020-10-14T01:22:36.278180Z"
    },
    "id": "tXSGrjWZ-llW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 5s 6ms/step - loss: 0.6568 - binary_accuracy: 0.7005 - val_loss: 0.5978 - val_binary_accuracy: 0.7802\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.5252 - binary_accuracy: 0.8117 - val_loss: 0.4744 - val_binary_accuracy: 0.8294\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.4184 - binary_accuracy: 0.8535 - val_loss: 0.3992 - val_binary_accuracy: 0.8554\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3542 - binary_accuracy: 0.8733 - val_loss: 0.3571 - val_binary_accuracy: 0.8648\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3132 - binary_accuracy: 0.8849 - val_loss: 0.3315 - val_binary_accuracy: 0.8718\n",
      "Epoch 6/10\n",
      "290/625 [============>.................] - ETA: 1s - loss: 0.2903 - binary_accuracy: 0.8929"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks= tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "out_v = io.open('vectors2.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata2.tsv', 'w', encoding='utf-8')\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EEGuDVuzb5r",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## True accuracy \n",
    "* Now we can use our testing data set. We will asses the loss and accuracy of the model, this should be worse than our loss and binary accuracy in the fitting. \n",
    "* The larger this difference the larger our overfitting. \n",
    "* Recall loss is like Chi-Sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:22:36.285269Z",
     "iopub.status.busy": "2020-10-14T01:22:36.284561Z",
     "iopub.status.idle": "2020-10-14T01:22:39.192167Z",
     "shell.execute_reply": "2020-10-14T01:22:39.191519Z"
    },
    "id": "zOMKywn4zReN",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldbQqCw2Xc1W",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "* The history object is basically the chains we stored in class. We can use it to plot accuracy or loss over time. \n",
    "* val_ denotes the validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:22:39.221449Z",
     "iopub.status.busy": "2020-10-14T01:22:39.220241Z",
     "iopub.status.idle": "2020-10-14T01:22:39.342622Z",
     "shell.execute_reply": "2020-10-14T01:22:39.343075Z"
    },
    "id": "2SEMeQ5YXs8z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "fig,axs=plt.subplots()\n",
    "fig.set_size_inches(11,8)\n",
    "axs.plot(epochs, loss, 'ko', label='Training loss')\n",
    "axs.plot(epochs, val_loss, 'k', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "plt.legend(loc='center')\n",
    "\n",
    "\n",
    "axs1 = axs.twinx()\n",
    "\n",
    "axs1.plot(epochs, acc, 'ro', label='Training acc')\n",
    "axs1.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "\n",
    "axs1.set_ylabel('Accuracy',color='r')\n",
    "axs1.tick_params(axis='y', labelcolor='r')\n",
    "plt.legend(loc='center right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-to23J3Vy5d3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Export the model\n",
    "\n",
    "* The final step is to add the vecotrization layer to the model and also add a sigmoid at the end to translate the ouput into a probability. This is done using the Sequential command. We can test this now on the raw strings we extracted earlier. The accuracy should be the same. We can also test it on user input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:22:39.483972Z",
     "iopub.status.busy": "2020-10-14T01:22:39.482850Z",
     "iopub.status.idle": "2020-10-14T01:22:43.883611Z",
     "shell.execute_reply": "2020-10-14T01:22:43.883086Z"
    },
    "id": "FWXsMvryuZuq",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n",
    "\n",
    "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwQgoN88LoEF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using the model\n",
    "\n",
    "Now the model can be used as a function where any string can be added and the classifier should be able to tell you if the movie was good or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:22:43.887975Z",
     "iopub.status.busy": "2020-10-14T01:22:43.887286Z",
     "iopub.status.idle": "2020-10-14T01:22:44.052893Z",
     "shell.execute_reply": "2020-10-14T01:22:44.052374Z"
    },
    "id": "QW355HH5L49K",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "examples=[str(input())]\n",
    "export_model.predict(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2017 François Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
